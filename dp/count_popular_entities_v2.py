"""
Counts wikititles and sorts them by frequency.

Arguments: 
1. path to folder generated by wikiextractor eswiki/eswiki-20170420_with_links_nodisamb/
2. redirect map as redirect --> title map. eg. eswiki-20170420_with_links_nodisamb.r2t
"""
from __future__ import print_function

import argparse
import logging
from urllib import parse
from collections import Counter

from bs4 import BeautifulSoup
from dp.title_normalizer import TitleNormalizer

logging.basicConfig(format='%(asctime)s: %(filename)s:%(lineno)d: %(message)s', level=logging.INFO)
from utils.misc_utils import load_redirects, load_id2title
from processors.basic_page_processor import BasicPageProcessor
import utils.constants as K
import sys


class EntityCounter(BasicPageProcessor):
    def __init__(self, wikipath, linksout, contsout, redirect_map, t2id, debug=False, limit=500000):
        super(EntityCounter, self).__init__(wikipath)
        self.counts = Counter()
        self.links = open(linksout, "w", encoding='utf-8')
        self.contsout = contsout
        self.t2id = t2id
        self.redirect_map = redirect_map
        self.null_counts = 0
        self.total_counts = 0
        self.limit = limit
        self.normalizer = TitleNormalizer(redirect_map=self.redirect_map, t2id=self.t2id)
        self.debug = debug
        self.bs4_failures = 0

    def after_dir_hook(self):
        logging.info("saw %d nulls %d total", self.null_counts, self.total_counts)
        if self.debug:
            self.finish()
            sys.exit(0)

    def process_wikicontent(self, page_content, page_id, page_title):
        try:
            soup = BeautifulSoup(page_content, "html.parser")
        except NotImplementedError as e:
            self.bs4_failures += 1
            logging.info("bs4 failed %d times", self.bs4_failures)
            return
        for outlink in soup.find_all("a"):
            try:
                out_title = parse.unquote(outlink["href"])  # unquote('abc%20def') -> 'abc def'.
                out_title = out_title.replace(" ", "_")
                if "#" in out_title:
                    idx = out_title.rfind("#")
                    out_title = out_title[:idx]
                text = outlink.text
                # logging.info(out_title)
                out_title = self.normalizer.normalize(title=out_title)
                # logging.info(out_title)
                self.total_counts += 1
                if out_title == K.NULL_TITLE:
                    self.null_counts += 1
                    continue
                self.links.write(text + "\t" + out_title + "\n")
                self.counts.update([out_title])
            except KeyError:
                logging.info("not found! %s", outlink)
                continue

    def finish(self):
        self.links.close()
        for title in self.t2id:
            if title not in self.counts:
                self.counts[title] = 0

        with open(self.contsout, "w", encoding='utf-8') as out:
            for idx, (title, cnt) in enumerate(self.counts.most_common()):
                if title not in self.t2id:
                    logging.info("did not find title %s in t2id", title)
                    continue
                buf = "%s\t%s\t%d\n" % (self.t2id[title], title, cnt)
                out.write(buf)
                # if idx > self.limit:
                #     break


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='entity linker')
    parser.add_argument('--wikitext', type=str, required=True, help='interactive')
    parser.add_argument('--id2title', type=str, required=True, help='interactive')
    parser.add_argument('--redirects', type=str, required=True, help='interactive')
    parser.add_argument('--linksout', type=str, required=True, help='file to write surface --> title information.')
    parser.add_argument('--contsout', type=str, required=True, help='file to write sorted counts in.')
    parser.add_argument('--debug', action="store_true", help='interactive')
    args = parser.parse_args()
    args = vars(args)

    # args = docopt("""Count popular entities, after normalizing redirects.
    #
    # Usage:
    #     script.py <wikipath> <id2title> <redirect_map> <out> <linksout> [--debug]
    #
    #     <wikipath> = path to wikipedia text generated from wikiextractor. eg. eswiki/eswiki-20170420_with_links_nodisamb/
    #     <redirect_map> = tsv file with redirect --> title map. eg. eswiki-20170420_with_links_nodisamb.r2t
    #     <out> = file to write sorted counts in.
    #     <linksout> = file to write surface --> title information.
    #
    # Options:
    # --debug  whether to stop after AA for debugging.
    # """)
    redirect_map = load_redirects(args["redirects"])
    _, t2id, _ = load_id2title(args["id2title"])
    p = EntityCounter(wikipath=args["wikitext"],
                      linksout=args["linksout"],
                      contsout=args["contsout"],
                      redirect_map=redirect_map, t2id=t2id,
                      debug=args["debug"], limit=0)
    p.run()
