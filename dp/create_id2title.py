# coding=utf-8
"""
Walks the directory generated by wikiextractor and creates a map from id to title.


Arguments: 
1. path to folder generated by wikiextractor
2. output filename. Output is tsv file with id and title as columns.
"""
from __future__ import print_function
import argparse
import logging
import os
import sys
from processors.basic_page_processor import BasicPageProcessor
import gzip

logging.basicConfig(format='%(asctime)s: %(filename)s:%(lineno)d: %(message)s', level=logging.INFO)


class TitleProcessor(BasicPageProcessor):
    def __init__(self, wikipath, outpath):
        super(TitleProcessor, self).__init__(wikipath)
        self.out = open(outpath, "w")

    def process_wikicontent(self, page_content, page_id, page_title):
        buf = "\t".join([page_id, page_title])
        # buf = buf.encode("utf-8")
        self.out.write(buf + "\n")

    def finish(self):
        self.out.close()


def read_id2title(wikiprefix, encoding, outpath):
    """
    page id in fr wikipedia --> target_lang (usually en) wikipedia title
    """
    filename = wikiprefix + "-page.sql.gz"
    logging.info("Reading id2title sql" + filename)
    f = gzip.open(filename, "rt", encoding=encoding)
    bad = 0
    with open(outpath, "w") as out:
        if "zhwiki" in wikiprefix:
            buf = "\t".join(["9999999", "杰布·布什", "1"])
            out.write(buf + "\n")

        for line in f:
            if "INSERT INTO" not in line:
                continue
            # (id,ns,title,restrictions,counter,is_redirect,is_new,...),(id,ns,...)
            start = line.index("(")
            line = line[start + 1:]
            parts = line.split("),(")
            for part in parts:
                id_ns, page_title, tmp = part.split(",'")[:3]
                page_id, ns = id_ns.split(",")
                # only ns 0 is genuine page, others are discussions, category pages etc.
                if ns != "0":
                    continue
                if len(tmp.split(",")) != 5:
                    logging.info("bad#%d %s", bad, part)
                    bad += 1
                    continue
                restrictions, counter, is_redirect, is_new, page_random = tmp.split(",")
                page_title = page_title[:len(page_title) - 1]
                if "\\" in page_title:
                    page_title = page_title.replace("\\", "")
                # print(part)
                buf = "\t".join([page_id, page_title, is_redirect])
                # buf = "\t".join([page_id, page_title])
                out.write(buf + "\n")


def load_disambiguation_ids(wikipath):
    logging.info("loading disamb title ids")
    disamb_ids = []
    for idx, line in enumerate(open(os.path.join(wikipath, "output", "page_categories.disamb"))):
        parts = line.split("\t")  # this is faster than maxsplit=3
        page_id = parts[0]
        disamb_ids.append(page_id)
    disamb_ids = set(disamb_ids)
    return disamb_ids


def id2title_from_datamachine(wikipath, outpath):
    disamb_ids = load_disambiguation_ids(wikipath=wikipath)
    disamb_id2title = {}
    logging.info("starting ...")
    with open(outpath, "w") as out:
        for idx, line in enumerate(open(os.path.join(wikipath, "output", "Page.txt"))):
            parts = line.split("\t")  # this is faster than maxsplit=3
            page_id = parts[0]

            page_title = parts[2]
            if "\\" in page_title:
                # fixes Master\'s_degree in titles
                page_title = page_title.replace("\\", "")

            if page_id in disamb_ids:
                disamb_id2title[page_id] = page_title

            buf = "\t".join([page_id, page_title])
            out.write(buf + "\n")
            if idx > 0 and idx % 100000 == 0:
                logging.info("read %d lines", idx)

    logging.info("writing disamb title map ...")
    disamb_path = os.path.splitext(outpath)[0] + ".disamb2t"
    with open(disamb_path, "w") as out:
        for did in disamb_id2title:
            d_title = disamb_id2title[did]
            buf = "\t".join([did, d_title])
            out.write(buf + "\n")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Create a page id to title map.')
    parser.add_argument('--wiki', type=str, required=True,
                        help='path to folder containing output/ from datamachine. eg. eswiki')
    parser.add_argument('--out', type=str, required=True, help='tsv file to write the map in. eg. eswiki-20170420.id2t')
    args = parser.parse_args()
    args = vars(args)
    logging.info("reading id2title from sql file")
    read_id2title(wikiprefix=args["wiki"], outpath=args["out"], encoding="utf-8")
    # logging.info("making id2title from datamachine ....")
    # id2title_from_datamachine(args["wiki"], args["out"])
    # logging.info("making id2title from wikiex output ....")
    # p = TitleProcessor(args["wiki"], args["out"]+".wikiex")
    # p.run()
